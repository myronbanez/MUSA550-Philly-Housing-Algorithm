{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Predictive Modeling of Housing Prices in Philadelphia\n",
    "**Due date: Wednesday, 12/7 by the end of the day**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Final Proposal - Mia Cherayil, Kendra Hills, Myron BaÃ±ez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposal can be found in the Github repository for this assignment under the file name \n",
    "\"550 Final Proposal - Mia, Kendra, Myron.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling Philadelphia's Housing Prices and Algorithmic Fairness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import carto2gpd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "pd.options.display.max_columns = 999\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load data from the Office of Property Assessment\n",
    "\n",
    "Use `carto2gpd` to load data for **single-family** properties in Philadelphia that had their **last sale during 2021**.\n",
    "\n",
    "Sources: \n",
    "- [OpenDataPhilly](https://www.opendataphilly.org/dataset/opa-property-assessments)\n",
    "- [Metadata](http://metadata.phila.gov/#home/datasetdetails/5543865f20583086178c4ee5/representationdetails/55d624fdad35c7e854cb21a4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carto_url = \"https://phl.carto.com/api/v2/sql\"\n",
    "\n",
    "# The table name\n",
    "table_name = \"opa_properties_public\"\n",
    "\n",
    "# Only pull 2021 sales for single family residential properties\n",
    "where = \"sale_date >= '2021-01-01' and sale_date <= '2021-12-31'\"\n",
    "where = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n",
    "\n",
    "# Run the query\n",
    "salesRaw = carto2gpd.get(carto_url, table_name, where=where)\n",
    "salesRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature columns being used\n",
    "cols = [\n",
    "    \"sale_price\",\n",
    "    \"total_livable_area\",\n",
    "    \"total_area\",\n",
    "    \"garage_spaces\",\n",
    "    \"fireplaces\",\n",
    "    \"number_of_bathrooms\",\n",
    "    \"number_of_bedrooms\",\n",
    "    \"number_stories\",\n",
    "    \"exterior_condition\",\n",
    "    \"zip_code\",\n",
    "    \"geometry\"\n",
    "]\n",
    "\n",
    "# Triming to these columns and removing NaNs\n",
    "sales = salesRaw[cols].dropna()\n",
    "\n",
    "# Trimming zip code to first five digits\n",
    "sales['zip_code'] = sales['zip_code'].astype(str).str.slice(0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Load data for census tracts and neighborhoods\n",
    "\n",
    "Load various Philadelphia-based regions we will use in our analysis.\n",
    "\n",
    "- Census tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\n",
    "- Neighborhoods can be downloaded from:\n",
    "https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts = gpd.read_file(\"Census_Tracts_2010.geojson\")\n",
    "tracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Spatially join the sales data and neighborhoods/census tracts.\n",
    "\n",
    "Perform a spatial join, such that each sale has an associated neighborhood and census tract.\n",
    "\n",
    "**Note:** after performing the first spatial join, you will need to use the `drop()` function to remove the `index_right` column; otherwise an error will be raised on the second spatial join about duplicate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales.to_crs(epsg=3857)\n",
    "tracts = tracts.to_crs(epsg=3857)\n",
    "phl_sales = gpd.sjoin(sales, tracts, predicate='within', how='inner').drop(columns=['index_right'])\n",
    "\n",
    "phl_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Train a Random Forest on the sales data\n",
    "\n",
    "You should follow the steps outlined in lecture to preprocess and train your model. \n",
    "\n",
    "**Extra credit: the students with the top 3 scores on the test set will receive extra credit (first place +3, second place +2, third place +1)**\n",
    "\n",
    "**Requirements**\n",
    "- Trim the sales data to those sales with prices between \\\\$3,000 and \\\\$1 million DONE\n",
    "- Set up a pipeline that includes both numerical columns and categorical columns DONE\n",
    "- Include one-hot encoded variables for the neighborhood of the sale DONE\n",
    "- Use a 70/30% training/test split DONE\n",
    "- Use GridSearchCV to perform a $k$-fold cross validation that optimize *at least 2* hyperparameters of the RandomForestRegressor\n",
    "- After fitting your model and finding the optimal hyperparameters, you should evaluate the score ($R^2$) on the test set (the original 30% sample withheld)\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- You are welcome to include additional features or perform any feature engineering that you want to try to improve the test accuracy\n",
    "- You can also experiment with the prediction variable, e.g., try predicting sale price per sq ft. (or its log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim Data\n",
    "valid = (phl_sales['sale_price'] > 3000) & (phl_sales['sale_price'] < 1e6)\n",
    "phl_sales = phl_sales.loc[valid]\n",
    "\n",
    "phl_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline + One-Hot Encoder\n",
    "\n",
    "# Numerical Columns\n",
    "num_cols = [\n",
    "    \"total_livable_area\",\n",
    "    \"total_area\",\n",
    "    \"garage_spaces\",\n",
    "    \"fireplaces\",\n",
    "    \"number_of_bathrooms\",\n",
    "    \"number_of_bedrooms\",\n",
    "    \"number_stories\",\n",
    "] \n",
    "\n",
    "# Categorical Columns\n",
    "cat_cols = [\"exterior_condition\", \"zip_code\"]\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    transformer, RandomForestRegressor(n_estimators=10, \n",
    "                                       random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data 70/30\n",
    "train_set, test_set = train_test_split(phl_sales, test_size=0.3, random_state=42)\n",
    "\n",
    "y_train = np.log(train_set[\"sale_price\"])\n",
    "y_test = np.log(test_set[\"sale_price\"])\n",
    "\n",
    "# Fit\n",
    "pipe.fit(train_set, y_train);\n",
    "\n",
    "# Score\n",
    "pipe.score(test_set, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "model_step = \"randomforestregressor\"\n",
    "param_grid = {\n",
    "    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30],\n",
    "    f\"{model_step}__max_depth\": [None, 2, 5, 7, 9, 13],\n",
    "}\n",
    "\n",
    "param_grid\n",
    "\n",
    "# Create the grid and use 3-fold CV\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\n",
    "\n",
    "# Run the search\n",
    "grid.fit(train_set, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Estimator\n",
    "grid.best_estimator_\n",
    "\n",
    "# Best Parameter\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = grid.best_estimator_\n",
    "grid.score(test_set, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Calculate the percent error of your model predictions for each sale in the test set\n",
    "\n",
    "Fit your best model and use it to make predictions on the test set.\n",
    "\n",
    "**Note:** this should be the percent error in terms of **sale price** or **sale price per sq ft**. You'll need to convert if you predicted the log!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = phl_sales.loc[test_set.index]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "log_prediction = grid.best_estimator_.predict(test_set)\n",
    "\n",
    "# Conversion from Log\n",
    "data['prediction'] = np.exp(log_prediction)\n",
    "\n",
    "# Percent Error\n",
    "data['pct_error'] = ((data[\"sale_price\"]/data[\"prediction\"])/data[\"sale_price\"]) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Make a data frame with percent errors and census tract info for each sale in the test set\n",
    "\n",
    "Create a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- When using the \"train_test_split()\" function, the index of the test data frame includes the labels from the original sales data frame\n",
    "- You can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\n",
    "- Add a new column to this data frame holding the percent error data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I believe that all the information needed for this section was already included in my dataframe.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Plot a map of the median percent error by census tract \n",
    "\n",
    "- You'll want to group your data frame of test sales by the `GEOID10` column and take the median of you percent error column\n",
    "- Merge the census tract geometries back in and use geopandas to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_tracts = data.groupby('GEOID10')\n",
    "\n",
    "# Median Percent Error Column \n",
    "mpe = census_tracts['pct_error'].median()\n",
    "\n",
    "# Merge Census Tracts\n",
    "mpe = tracts.merge(mpe, on='GEOID10')\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "mpe.plot(ax=ax, \n",
    "                 column='pct_error',\n",
    "                 cmap = 'inferno',\n",
    "                 legend=True)\n",
    "ax.set_title(\"Median Percent Error by Philly Census Tracts\", fontsize=16)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Compare the percent errors in Qualifying Census Tracts and other tracts \n",
    "\n",
    "[Qualifying Census Tracts](https://www.huduser.gov/portal/datasets/qct.html) are a poverty designation that HUD uses to allocate housing tax credits\n",
    "\n",
    "- I've included a list of the census tract names that qualify in Philadelphia\n",
    "- Add a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\n",
    "- Then, group by this new column and calculate the median percent error\n",
    "\n",
    "**You should find that the algorithm's accuracy is significantly worse in these low-income, qualifying census tracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct = ['5',\n",
    " '20',\n",
    " '22',\n",
    " '28.01',\n",
    " '30.01',\n",
    " '30.02',\n",
    " '31',\n",
    " '32',\n",
    " '33',\n",
    " '36',\n",
    " '37.01',\n",
    " '37.02',\n",
    " '39.01',\n",
    " '41.01',\n",
    " '41.02',\n",
    " '56',\n",
    " '60',\n",
    " '61',\n",
    " '62',\n",
    " '63',\n",
    " '64',\n",
    " '65',\n",
    " '66',\n",
    " '67',\n",
    " '69',\n",
    " '70',\n",
    " '71.01',\n",
    " '71.02',\n",
    " '72',\n",
    " '73',\n",
    " '74',\n",
    " '77',\n",
    " '78',\n",
    " '80',\n",
    " '81.01',\n",
    " '81.02',\n",
    " '82',\n",
    " '83.01',\n",
    " '83.02',\n",
    " '84',\n",
    " '85',\n",
    " '86.01',\n",
    " '86.02',\n",
    " '87.01',\n",
    " '87.02',\n",
    " '88.01',\n",
    " '88.02',\n",
    " '90',\n",
    " '91',\n",
    " '92',\n",
    " '93',\n",
    " '94',\n",
    " '95',\n",
    " '96',\n",
    " '98.01',\n",
    " '100',\n",
    " '101',\n",
    " '102',\n",
    " '103',\n",
    " '104',\n",
    " '105',\n",
    " '106',\n",
    " '107',\n",
    " '108',\n",
    " '109',\n",
    " '110',\n",
    " '111',\n",
    " '112',\n",
    " '113',\n",
    " '119',\n",
    " '121',\n",
    " '122.01',\n",
    " '122.03',\n",
    " '131',\n",
    " '132',\n",
    " '137',\n",
    " '138',\n",
    " '139',\n",
    " '140',\n",
    " '141',\n",
    " '144',\n",
    " '145',\n",
    " '146',\n",
    " '147',\n",
    " '148',\n",
    " '149',\n",
    " '151.01',\n",
    " '151.02',\n",
    " '152',\n",
    " '153',\n",
    " '156',\n",
    " '157',\n",
    " '161',\n",
    " '162',\n",
    " '163',\n",
    " '164',\n",
    " '165',\n",
    " '167.01',\n",
    " '167.02',\n",
    " '168',\n",
    " '169.01',\n",
    " '169.02',\n",
    " '170',\n",
    " '171',\n",
    " '172.01',\n",
    " '172.02',\n",
    " '173',\n",
    " '174',\n",
    " '175',\n",
    " '176.01',\n",
    " '176.02',\n",
    " '177.01',\n",
    " '177.02',\n",
    " '178',\n",
    " '179',\n",
    " '180.02',\n",
    " '188',\n",
    " '190',\n",
    " '191',\n",
    " '192',\n",
    " '195.01',\n",
    " '195.02',\n",
    " '197',\n",
    " '198',\n",
    " '199',\n",
    " '200',\n",
    " '201.01',\n",
    " '201.02',\n",
    " '202',\n",
    " '203',\n",
    " '204',\n",
    " '205',\n",
    " '206',\n",
    " '208',\n",
    " '239',\n",
    " '240',\n",
    " '241',\n",
    " '242',\n",
    " '243',\n",
    " '244',\n",
    " '245',\n",
    " '246',\n",
    " '247',\n",
    " '249',\n",
    " '252',\n",
    " '253',\n",
    " '265',\n",
    " '267',\n",
    " '268',\n",
    " '271',\n",
    " '274.01',\n",
    " '274.02',\n",
    " '275',\n",
    " '276',\n",
    " '277',\n",
    " '278',\n",
    " '279.01',\n",
    " '279.02',\n",
    " '280',\n",
    " '281',\n",
    " '282',\n",
    " '283',\n",
    " '284',\n",
    " '285',\n",
    " '286',\n",
    " '287',\n",
    " '288',\n",
    " '289.01',\n",
    " '289.02',\n",
    " '290',\n",
    " '291',\n",
    " '293',\n",
    " '294',\n",
    " '298',\n",
    " '299',\n",
    " '300',\n",
    " '301',\n",
    " '302',\n",
    " '305.01',\n",
    " '305.02',\n",
    " '309',\n",
    " '311.01',\n",
    " '312',\n",
    " '313',\n",
    " '314.01',\n",
    " '314.02',\n",
    " '316',\n",
    " '318',\n",
    " '319',\n",
    " '321',\n",
    " '325',\n",
    " '329',\n",
    " '330',\n",
    " '337.01',\n",
    " '345.01',\n",
    " '357.01',\n",
    " '376',\n",
    " '377',\n",
    " '380',\n",
    " '381',\n",
    " '382',\n",
    " '383',\n",
    " '389',\n",
    " '390']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True/False Column of QCT\n",
    "median['QCT'] = np.where(median['NAME10'].isin(qct), True, False)\n",
    "\n",
    "# Group By and Calculation\n",
    "median_byqct = median.groupby('QCT')\n",
    "medianqct = median_byqct['pct_error'].median()\n",
    "\n",
    "medianqct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
